akka {

  loggers = ["akka.event.slf4j.Slf4jLogger"]
  # backend(logback, log4j) 보다 우선이다. 해당 레벨에서 필터링 후 backend로 로그가 넘어간다.
  loglevel = "INFO"
  stdout-loglevel = "DEBUG"
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  actor {
    provider = "cluster"

    deployment {
      /serviceRouter {
        router = round-robin-group
        routees.paths = ["/user/w1"]
        cluster {
          enabled = on
          allow-local-routees = on
          #use-role = compute
        }
      }
    }


  }
  remote {

    # 기본값은 on 이다. inbound message 오류시 이걸 사용하거나 remote Artery(새로운 remote)를 사용하는 것이 좋다.
    use-passive-connections=off
    log-remote-lifecycle-events = off
    netty.tcp {
      #  hostname = "127.0.0.1"
      #  port = 0
    }
  }

  ## 영속화 설정
  persistence {

    journal {
      plugin = "akka.persistence.journal.leveldb"
      auto-start-journals = ["akka.persistence.journal.leveldb"]
      leveldb {
        dir = "journal"
      }
    }

    snapshot-store {
      plugin = "akka.persistence.snapshot-store.local"
      auto-start-snapshot-stores = ["akka.persistence.snapshot-store.local"]
      local{
        dir="snapshots"
      }

    }

  }


  cluster {
    // 강제 연결
    configuration-compatibility-check.enforce-on-join=off

    quarantine-removed-node-after = 5s

    down-removal-margin = 10s
    #down-removal-margin = off

    allow-weakly-up-members = on

    #seed-nodes = [
    # "akka.tcp://ClusterSystem@127.0.0.1:2551",
    # "akka.tcp://ClusterSystem@127.0.0.1:2552"
    #   ]
    singleton{
      # When a node is becoming oldest it sends hand-over request to previous oldest,
      # that might be leaving the cluster. This is retried with this interval until
      # the previous oldest confirms that the hand over has started or the previous
      # oldest member is removed from the cluster (+ akka.cluster.down-removal-margin).
      hand-over-retry-interval = 1s
      # The number of retries are derived from hand-over-retry-interval and
      # akka.cluster.down-removal-margin (or ClusterSingletonManagerSettings.removalMargin),
      # but it will never be less than this property.
      min-number-of-hand-over-retries = 10
    }

    min-nr-of-members = 1
    ##min-nr-of-members = 2

    # auto downing is NOT safe for production deployments.
    # you may want to use it during development, read more about it in the docs.
    # 연결 되지 않는 node는 down 상태로 만든다. 이후 네트워크 연결되어도 클러스터에 붙지 않는다.
    # auto-down-unreachable-after = 10s

  }
}


# 다운결정자 https://developer.lightbend.com/docs/akka-commercial-addons/current/split-brain-resolver.html
#
#akka.cluster.downing-provider-class = "com.lightbend.akka.sbr.SplitBrainResolverProvider"
akka.cluster.split-brain-resolver {
  # Select one of the available strategies (see descriptions below):
  # static-quorum, keep-majority, keep-oldest, keep-referee
  # if left "off" when the downing provider is enabled cluster startup will fail.
  active-strategy = static-quorum

  # Time margin after which shards or singletons that belonged to a downed/removed
  # partition are created in surviving partition. The purpose of this margin is that
  # in case of a network partition the persistent actors in the non-surviving partitions
  # must be stopped before corresponding persistent actors are started somewhere else.
  # This is useful if you implement downing strategies that handle network partitions,
  # e.g. by keeping the larger side of the partition and shutting down the smaller side.
  # Decision is taken by the strategy when there has been no membership or
  # reachability changes for this duration, i.e. the cluster state is stable.
  stable-after = 20s

  tatic-quorum {
    # minimum number of nodes that the cluster must have
    quorum-size = 1

    # if the 'role' is defined the decision is based only on members with that 'role'
    role = ""
  }
  keep-majority {
    # if the 'role' is defined the decision is based only on members with that 'role'
    role = ""
  }
  keep-oldest {
    # Enable downing of the oldest node when it is partitioned from all other nodes
    down-if-alone = on

    # if the 'role' is defined the decision is based only on members with that 'role',
    # i.e. using the oldest member (singleton) within the nodes with that role
    role = ""
  }
  keep-referee {
    # referee address on the form of "akka.tcp://system@hostname:port"
    address = ""
    down-all-if-less-than-nodes = 1
  }
}


# 다운 결정자 simple-akka-downing
# group: 'com.ajjpj.simple-akka-downing', name: 'simple-akka-downing_2.12', version: '0.9.2'
#akka.cluster.downing-provider-class = com.ajjpj.simpleakkadowning.SimpleAkkaDowningProvider
simple-akka-downing {
  # Time margin after which shards or singletons that belonged to a downed/removed
  #  partition are created in surviving partition. The purpose of this margin is that
  #  in case of a network partition the persistent actors in the non-surviving partitions
  #  must be stopped before corresponding persistent actors are started somewhere else.
  #
  # This margin should be picked based on cluster size, a larger number of nodes introducing
  #  more variance in timing.
  #
  # Disable with "off" or specify a duration to enable.
  #
  # See akka.cluster.down-removal-margin
  #down-removal-margin = 5s
  down-removal-margin = off

  # Time margin after which unreachable nodes in a stable cluster state (i.e. no nodes changed
  #  their membership state or their reachability) are treated as permanently unreachable, and
  #  the split-brain resolution strategy kicks in.
  stable-after = 10s


  # The active strategy is one of static-quorum, keep-majority and keep-oldest. It is triggered
  #  after the cluster configuration has been stable for an interval of 'stable-after'.
  #
  # static-quorum defines a fixed number of nodes, and a network partition must have at least
  #  this number of reachable nodes (in a given role, if that is specified) in order to be allowed
  #  to survive. If the quorum size is picked bigger than half the maximum number of cluster nodes,
  #  this strategy is completely robust. It does not however work well with a dynamically growing
  #  (or shrinking) cluster.
  #
  # keep-majority uses the number of cluster nodes as the baseline and requires a network partition
  #  to have more than half that number of (reachable) nodes in order to be allowed to survive. This
  #  fully supports elastically growing and shrinking clusters, but there are rare race conditions
  #  that can lead to both partitions to be downed or - potentially worse - both partitions to survive.
  #
  # keep-oldest requires the oldest member to be part of a partition for it to survive. This can be
  #  useful since the oldest node is where cluster singletons are running, so this strategy does not
  #  singletons to be migrated and restarted. It reliably prevents split brain, but it can lead to
  #  a situation where 2 nodes survive and 25 nodes are downed. To deal with the pathological special
  #  case that the oldest node is in a network partition of its own, the flag 'down-if-alone' can be
  #  used to specify the oldest node if it is all by itself.
  active-strategy = static-quorum

  # If initial cluster startup delay causes network partitions to be detected, setting
  #  akka.cluster.min-nr-or-members to the value of quorum-size can delay checks
  static-quorum {
    # minimum number of nodes that the cluster must have
    quorum-size = 1

    # if the 'role' is defined the decision is based only on members with that 'role'
    role = ""
  }

  keep-majority {
    role = ""
  }

  keep-oldest {
    # if on, activates special treatment for the situation that the oldest node is the only node to
    #  be split off, causing it to be downed and the rest of the cluster to survive.
    down-if-alone = off
  }
}


#compile group: 'org.guangwenz', name: 'akka-down-resolver_2.12', version: '1.2.4'
#akka.cluster.downing-provider-class = "org.guangwenz.akka.cluster.SplitBrainResolver"
guangwenz.cluster.split-brain-resolver {
  active-strategy = static-quorum
  #the time to wait before resolving the split brain situation.
  stable-after = 7s

  static-quorum {
    # N / 2 + 1 is the recommend settings for this quorum size.
    quorum-size = 1
  }
}

#   compile group: 'pl.immutables', name: 'akka-reasonable-downing', version: '1.1.0'
#akka.cluster.downing-provider-class = "pl.immutables.akka.reasonable.downing.StaticQuorumDowningProvider"
akka{
  reasonable.downing {
    # the time to make a decision after the cluster is stable
    stable-after = 7 seconds

    # the N / 2 + 1 where N is number of nodes in a static cluster
    quorum-size = 1

    # list of the roles which be used in quorum. may be empty or absent.
    quorum-roles = ["seed"]
  }


}






# Enable metrics extension in akka-cluster-metrics.
akka.extensions=["akka.cluster.metrics.ClusterMetricsExtension"]
akka.extensions = ["akka.cluster.client.ClusterClientReceptionist"]

# Sigar native library extract location during tests.
# Note: use per-jvm-instance folder when running multiple jvm on one host.
akka.cluster.metrics.native-library-extract-folder=${user.dir}/target/native



# Settings for the ClusterShardingExtension
akka.cluster.sharding {

  # The extension creates a top level actor with this name in top level system scope,
  # e.g. '/system/sharding'
  guardian-name = sharding

  # Specifies that entities runs on cluster nodes with a specific role.
  # If the role is not specified (or empty) all nodes in the cluster are used.
  role = ""

  # When this is set to 'on' the active entity actors will automatically be restarted
  # upon Shard restart. i.e. if the Shard is started on a different ShardRegion
  # due to rebalance or crash.
  remember-entities = on

  # If the coordinator can't store state changes it will be stopped
  # and started again after this duration, with an exponential back-off
  # of up to 5 times this duration.
  coordinator-failure-backoff = 5 s

  # The ShardRegion retries registration and shard location requests to the
  # ShardCoordinator with this interval if it does not reply.
  retry-interval = 2 s

  # Maximum number of messages that are buffered by a ShardRegion actor.
  buffer-size = 100000

  # Timeout of the shard rebalancing process.
  handoff-timeout = 60 s

  # Time given to a region to acknowledge it's hosting a shard.
  shard-start-timeout = 10 s

  # If the shard is remembering entities and can't store state changes
  # will be stopped and then started again after this duration. Any messages
  # sent to an affected entity may be lost in this process.
  shard-failure-backoff = 10 s

  # If the shard is remembering entities and an entity stops itself without
  # using passivate. The entity will be restarted after this duration or when
  # the next message for it is received, which ever occurs first.
  entity-restart-backoff = 10 s

  # Rebalance check is performed periodically with this interval.
  rebalance-interval = 10 s

  # Absolute path to the journal plugin configuration entity that is to be
  # used for the internal persistence of ClusterSharding. If not defined
  # the default journal plugin is used. Note that this is not related to
  # persistence used by the entity actors.
  # Only used when state-store-mode=persistence
  journal-plugin-id = "persistence"

  # Absolute path to the snapshot plugin configuration entity that is to be
  # used for the internal persistence of ClusterSharding. If not defined
  # the default snapshot plugin is used. Note that this is not related to
  # persistence used by the entity actors.
  # Only used when state-store-mode=persistence
  snapshot-plugin-id = "persistence"

  # Defines how the coordinator stores its state. Same is also used by the
  # shards for rememberEntities.
  # Valid values are "ddata" or "persistence".
  state-store-mode = "ddata"

  # The shard saves persistent snapshots after this number of persistent
  # events. Snapshots are used to reduce recovery times.
  # Only used when state-store-mode=persistence
  snapshot-after = 1000

  # The shard deletes persistent events (messages and snapshots) after doing snapshot
  # keeping this number of old persistent batches.
  # Batch is of size `snapshot-after`.
  # When set to 0 after snapshot is successfully done all messages with equal or lower sequence number will be deleted.
  # Default value of 2 leaves last maximum 2*`snapshot-after` messages and 3 snapshots (2 old ones + fresh snapshot)
  keep-nr-of-batches = 2

  # Setting for the default shard allocation strategy
  least-shard-allocation-strategy {
    # Threshold of how large the difference between most and least number of
    # allocated shards must be to begin the rebalancing.
    rebalance-threshold = 10

    # The number of ongoing rebalancing processes is limited to this number.
    max-simultaneous-rebalance = 3
  }

  # Timeout of waiting the initial distributed state (an initial state will be queried again if the timeout happened)
  # Only used when state-store-mode=ddata
  waiting-for-state-timeout = 5 s

  # Timeout of waiting for update the distributed state (update will be retried if the timeout happened)
  # Only used when state-store-mode=ddata
  updating-state-timeout = 5 s

  # The shard uses this strategy to determines how to recover the underlying entity actors. The strategy is only used
  # by the persistent shard when rebalancing or restarting. The value can either be "all" or "constant". The "all"
  # strategy start all the underlying entity actors at the same time. The constant strategy will start the underlying
  # entity actors at a fix rate. The default strategy "all".
  entity-recovery-strategy = "all"

  # Default settings for the constant rate entity recovery strategy
  entity-recovery-constant-rate-strategy {
    # Sets the frequency at which a batch of entity actors is started.
    frequency = 100 ms
    # Sets the number of entity actors to be restart at a particular interval
    number-of-entities = 5
  }

  # Settings for the coordinator singleton. Same layout as akka.cluster.singleton.
  # The "role" of the singleton configuration is not used. The singleton role will
  # be the same as "akka.cluster.sharding.role".
  coordinator-singleton = ${akka.cluster.singleton}

  # Settings for the Distributed Data replicator.
  # Same layout as akka.cluster.distributed-data.
  # The "role" of the distributed-data configuration is not used. The distributed-data
  # role will be the same as "akka.cluster.sharding.role".
  # Note that there is one Replicator per role and it's not possible
  # to have different distributed-data settings for different sharding entity types.
  # Only used when state-store-mode=ddata
  distributed-data = ${akka.cluster.distributed-data}
  distributed-data {
    # minCap parameter to MajorityWrite and MajorityRead consistency level.
    majority-min-cap = 5
    durable.keys = ["*"]

    # When using many entities with "remember entities" the Gossip message
    # can become to large if including to many in same message. Limit to
    # the same number as the number of ORSet per shard.
    max-delta-elements = 5

  }

  # The id of the dispatcher to use for ClusterSharding actors.
  # If not specified default dispatcher is used.
  # If specified you need to define the settings of the actual dispatcher.
  # This dispatcher for the entity actors is defined by the user provided
  # Props, i.e. this dispatcher is not used for the entity actors.
  use-dispatcher = ""
}


akka.cluster.distributed-data {
  # Actor name of the Replicator actor, /system/ddataReplicator
  name = ddataReplicator

  # Replicas are running on members tagged with this role.
  # All members are used if undefined or empty.
  role = ""

  # How often the Replicator should send out gossip information
  gossip-interval = 2 s

  # How often the subscribers will be notified of changes, if any
  notify-subscribers-interval = 500 ms

  # Maximum number of entries to transfer in one gossip message when synchronizing
  # the replicas. Next chunk will be transferred in next round of gossip.
  max-delta-elements = 1000

  # The id of the dispatcher to use for Replicator actors. If not specified
  # default dispatcher is used.
  # If specified you need to define the settings of the actual dispatcher.
  use-dispatcher = ""

  # How often the Replicator checks for pruning of data associated with
  # removed cluster nodes. If this is set to 'off' the pruning feature will
  # be completely disabled.
  pruning-interval = 120 s

  # How long time it takes to spread the data to all other replica nodes.
  # This is used when initiating and completing the pruning process of data associated
  # with removed cluster nodes. The time measurement is stopped when any replica is
  # unreachable, but it's still recommended to configure this with certain margin.
  # It should be in the magnitude of minutes even though typical dissemination time
  # is shorter (grows logarithmic with number of nodes). There is no advantage of
  # setting this too low. Setting it to large value will delay the pruning process.
  max-pruning-dissemination = 300 s

  # The markers of that pruning has been performed for a removed node are kept for this
  # time and thereafter removed. If and old data entry that was never pruned is somehow
  # injected and merged with existing data after this time the value will not be correct.
  # This would be possible (although unlikely) in the case of a long network partition.
  # It should be in the magnitude of hours. For durable data it is configured by
  # 'akka.cluster.distributed-data.durable.pruning-marker-time-to-live'.
  pruning-marker-time-to-live = 6 h

  # Serialized Write and Read messages are cached when they are sent to
  # several nodes. If no further activity they are removed from the cache
  # after this duration.
  serializer-cache-time-to-live = 10s

  # Settings for delta-CRDT
  delta-crdt {
    # enable or disable delta-CRDT replication
    enabled = on

    # Some complex deltas grow in size for each update and above this
    # threshold such deltas are discarded and sent as full state instead.
    # This is number of elements or similar size hint, not size in bytes.
    max-delta-size = 200
  }

  durable {
    # List of keys that are durable. Prefix matching is supported by using * at the
    # end of a key.
    keys = ["*"]

    # The markers of that pruning has been performed for a removed node are kept for this
    # time and thereafter removed. If and old data entry that was never pruned is
    # injected and merged with existing data after this time the value will not be correct.
    # This would be possible if replica with durable data didn't participate in the pruning
    # (e.g. it was shutdown) and later started after this time. A durable replica should not
    # be stopped for longer time than this duration and if it is joining again after this
    # duration its data should first be manually removed (from the lmdb directory).
    # It should be in the magnitude of days. Note that there is a corresponding setting
    # for non-durable data: 'akka.cluster.distributed-data.pruning-marker-time-to-live'.
    pruning-marker-time-to-live = 10 d

    # Fully qualified class name of the durable store actor. It must be a subclass
    # of akka.actor.Actor and handle the protocol defined in
    # akka.cluster.ddata.DurableStore. The class must have a constructor with
    # com.typesafe.config.Config parameter.
    store-actor-class = akka.cluster.ddata.LmdbDurableStore

    use-dispatcher = akka.cluster.distributed-data.durable.pinned-store

    pinned-store {
      executor = thread-pool-executor
      type = PinnedDispatcher
    }

    # Config for the LmdbDurableStore
    lmdb {
      # Directory of LMDB file. There are two options:
      # 1. A relative or absolute path to a directory that ends with 'ddata'
      #    the full name of the directory will contain name of the ActorSystem
      #    and its remote port.
      # 2. Otherwise the path is used as is, as a relative or absolute path to
      #    a directory.
      #
      # When running in production you may want to configure this to a specific
      # path (alt 2), since the default directory contains the remote port of the
      # actor system to make the name unique. If using a dynamically assigned
      # port (0) it will be different each time and the previously stored data
      # will not be loaded.
      dir = "ddata"

      # Size in bytes of the memory mapped file.
      map-size = 100 MiB

      # Accumulate changes before storing improves performance with the
      # risk of losing the last writes if the JVM crashes.
      # The interval is by default set to 'off' to write each update immediately.
      # Enabling write behind by specifying a duration, e.g. 200ms, is especially
      # efficient when performing many writes to the same key, because it is only
      # the last value for each key that will be serialized and stored.
      # write-behind-interval = 200 ms
      write-behind-interval = off
    }
  }

}