akka {
  actor {
    provider = "cluster"
  

loggers = ["akka.event.slf4j.Slf4jLogger"]
 loglevel = "INFO"
    stdout-loglevel = "DEBUG"
      logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"
    
  }
  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      hostname = "127.0.0.1"
      port = 2552
    }
  }


  persistence {

    journal {
      plugin = "akka.persistence.journal.leveldb"
      auto-start-journals = ["akka.persistence.journal.leveldb"]
      leveldb {
        dir = "journal2"
      }
    }

    snapshot-store {
      plugin = "akka.persistence.snapshot-store.local"
      auto-start-snapshot-stores = ["akka.persistence.snapshot-store.local"]
      local{
        dir="snapshots2"
      }

    }

  }


  cluster {
    quarantine-removed-node-after = 5s

    down-removal-margin = 5s

    seed-nodes = [
      "akka.tcp://ClusterSystem@127.0.0.1:2551",
      "akka.tcp://ClusterSystem@127.0.0.1:2552",
      "akka.tcp://ClusterSystem@127.0.0.1:2553"
      ]
    min-nr-of-members = 1

    singleton{
      # When a node is becoming oldest it sends hand-over request to previous oldest,
      # that might be leaving the cluster. This is retried with this interval until
      # the previous oldest confirms that the hand over has started or the previous
      # oldest member is removed from the cluster (+ akka.cluster.down-removal-margin).
      hand-over-retry-interval = 1s
      # The number of retries are derived from hand-over-retry-interval and
      # akka.cluster.down-removal-margin (or ClusterSingletonManagerSettings.removalMargin),
      # but it will never be less than this property.
      min-number-of-hand-over-retries = 10
    }
    # auto downing is NOT safe for production deployments.
    # you may want to use it during development, read more about it in the docs.
    #
    # auto-down-unreachable-after = 10s

  }
}

akka.cluster.downing-provider-class = com.ajjpj.simpleakkadowning.SimpleAkkaDowningProvider
simple-akka-downing {
  # Time margin after which shards or singletons that belonged to a downed/removed
  #  partition are created in surviving partition. The purpose of this margin is that
  #  in case of a network partition the persistent actors in the non-surviving partitions
  #  must be stopped before corresponding persistent actors are started somewhere else.
  #
  # This margin should be picked based on cluster size, a larger number of nodes introducing
  #  more variance in timing.
  #
  # Disable with "off" or specify a duration to enable.
  #
  # See akka.cluster.down-removal-margin
  down-removal-margin = 10s

  # Time margin after which unreachable nodes in a stable cluster state (i.e. no nodes changed
  #  their membership state or their reachability) are treated as permanently unreachable, and
  #  the split-brain resolution strategy kicks in.
  stable-after = 5s

  # The active strategy is one of static-quorum, keep-majority and keep-oldest. It is triggered
  #  after the cluster configuration has been stable for an interval of 'stable-after'.
  #
  # static-quorum defines a fixed number of nodes, and a network partition must have at least
  #  this number of reachable nodes (in a given role, if that is specified) in order to be allowed
  #  to survive. If the quorum size is picked bigger than half the maximum number of cluster nodes,
  #  this strategy is completely robust. It does not however work well with a dynamically growing
  #  (or shrinking) cluster.
  #
  # keep-majority uses the number of cluster nodes as the baseline and requires a network partition
  #  to have more than half that number of (reachable) nodes in order to be allowed to survive. This
  #  fully supports elastically growing and shrinking clusters, but there are rare race conditions
  #  that can lead to both partitions to be downed or - potentially worse - both partitions to survive.
  #
  # keep-oldest requires the oldest member to be part of a partition for it to survive. This can be
  #  useful since the oldest node is where cluster singletons are running, so this strategy does not
  #  singletons to be migrated and restarted. It reliably prevents split brain, but it can lead to
  #  a situation where 2 nodes survive and 25 nodes are downed. To deal with the pathological special
  #  case that the oldest node is in a network partition of its own, the flag 'down-if-alone' can be
  #  used to specify the oldest node if it is all by itself.
  active-strategy = static-quorum

  # If initial cluster startup delay causes network partitions to be detected, setting
  #  akka.cluster.min-nr-or-members to the value of quorum-size can delay checks
  static-quorum {
    # minimum number of nodes that the cluster must have
    quorum-size = 1

    # if the 'role' is defined the decision is based only on members with that 'role'
    role = ""
  }

  keep-majority {
    role = ""
  }

  keep-oldest {
    # if on, activates special treatment for the situation that the oldest node is the only node to
    #  be split off, causing it to be downed and the rest of the cluster to survive.
    down-if-alone = on
  }
}



# Enable metrics extension in akka-cluster-metrics.
akka.extensions=["akka.cluster.metrics.ClusterMetricsExtension"]

# Sigar native library extract location during tests.
# Note: use per-jvm-instance folder when running multiple jvm on one host.
akka.cluster.metrics.native-library-extract-folder=${user.dir}/target/native



# Settings for the ClusterShardingExtension
akka.cluster.sharding {

  # The extension creates a top level actor with this name in top level system scope,
  # e.g. '/system/sharding'
  guardian-name = sharding

  # Specifies that entities runs on cluster nodes with a specific role.
  # If the role is not specified (or empty) all nodes in the cluster are used.
  role = ""

  # When this is set to 'on' the active entity actors will automatically be restarted
  # upon Shard restart. i.e. if the Shard is started on a different ShardRegion
  # due to rebalance or crash.
  remember-entities = on

  # If the coordinator can't store state changes it will be stopped
  # and started again after this duration, with an exponential back-off
  # of up to 5 times this duration.
  coordinator-failure-backoff = 5 s

  # The ShardRegion retries registration and shard location requests to the
  # ShardCoordinator with this interval if it does not reply.
  retry-interval = 2 s

  # Maximum number of messages that are buffered by a ShardRegion actor.
  buffer-size = 100000

  # Timeout of the shard rebalancing process.
  handoff-timeout = 60 s

  # Time given to a region to acknowledge it's hosting a shard.
  shard-start-timeout = 10 s

  # If the shard is remembering entities and can't store state changes
  # will be stopped and then started again after this duration. Any messages
  # sent to an affected entity may be lost in this process.
  shard-failure-backoff = 10 s

  # If the shard is remembering entities and an entity stops itself without
  # using passivate. The entity will be restarted after this duration or when
  # the next message for it is received, which ever occurs first.
  entity-restart-backoff = 10 s

  # Rebalance check is performed periodically with this interval.
  rebalance-interval = 10 s

  # Absolute path to the journal plugin configuration entity that is to be
  # used for the internal persistence of ClusterSharding. If not defined
  # the default journal plugin is used. Note that this is not related to
  # persistence used by the entity actors.
  # Only used when state-store-mode=persistence
  journal-plugin-id = ""

  # Absolute path to the snapshot plugin configuration entity that is to be
  # used for the internal persistence of ClusterSharding. If not defined
  # the default snapshot plugin is used. Note that this is not related to
  # persistence used by the entity actors.
  # Only used when state-store-mode=persistence
  snapshot-plugin-id = ""

  # Defines how the coordinator stores its state. Same is also used by the
  # shards for rememberEntities.
  # Valid values are "ddata" or "persistence".
  state-store-mode = "persistence"

  # The shard saves persistent snapshots after this number of persistent
  # events. Snapshots are used to reduce recovery times.
  # Only used when state-store-mode=persistence
  snapshot-after = 1000

  # The shard deletes persistent events (messages and snapshots) after doing snapshot
  # keeping this number of old persistent batches.
  # Batch is of size `snapshot-after`.
  # When set to 0 after snapshot is successfully done all messages with equal or lower sequence number will be deleted.
  # Default value of 2 leaves last maximum 2*`snapshot-after` messages and 3 snapshots (2 old ones + fresh snapshot)
  keep-nr-of-batches = 2

  # Setting for the default shard allocation strategy
  least-shard-allocation-strategy {
    # Threshold of how large the difference between most and least number of
    # allocated shards must be to begin the rebalancing.
    rebalance-threshold = 10

    # The number of ongoing rebalancing processes is limited to this number.
    max-simultaneous-rebalance = 3
  }

  # Timeout of waiting the initial distributed state (an initial state will be queried again if the timeout happened)
  # Only used when state-store-mode=ddata
  waiting-for-state-timeout = 5 s

  # Timeout of waiting for update the distributed state (update will be retried if the timeout happened)
  # Only used when state-store-mode=ddata
  updating-state-timeout = 5 s

  # The shard uses this strategy to determines how to recover the underlying entity actors. The strategy is only used
  # by the persistent shard when rebalancing or restarting. The value can either be "all" or "constant". The "all"
  # strategy start all the underlying entity actors at the same time. The constant strategy will start the underlying
  # entity actors at a fix rate. The default strategy "all".
  entity-recovery-strategy = "all"

  # Default settings for the constant rate entity recovery strategy
  entity-recovery-constant-rate-strategy {
    # Sets the frequency at which a batch of entity actors is started.
    frequency = 100 ms
    # Sets the number of entity actors to be restart at a particular interval
    number-of-entities = 5
  }

  # Settings for the coordinator singleton. Same layout as akka.cluster.singleton.
  # The "role" of the singleton configuration is not used. The singleton role will
  # be the same as "akka.cluster.sharding.role".
  coordinator-singleton = ${akka.cluster.singleton}

  # Settings for the Distributed Data replicator.
  # Same layout as akka.cluster.distributed-data.
  # The "role" of the distributed-data configuration is not used. The distributed-data
  # role will be the same as "akka.cluster.sharding.role".
  # Note that there is one Replicator per role and it's not possible
  # to have different distributed-data settings for different sharding entity types.
  # Only used when state-store-mode=ddata
  distributed-data = ${akka.cluster.distributed-data}
  distributed-data {
    # minCap parameter to MajorityWrite and MajorityRead consistency level.
    majority-min-cap = 5
    durable.keys = ["shard-*"]

    # When using many entities with "remember entities" the Gossip message
    # can become to large if including to many in same message. Limit to
    # the same number as the number of ORSet per shard.
    max-delta-elements = 5

  }

  # The id of the dispatcher to use for ClusterSharding actors.
  # If not specified default dispatcher is used.
  # If specified you need to define the settings of the actual dispatcher.
  # This dispatcher for the entity actors is defined by the user provided
  # Props, i.e. this dispatcher is not used for the entity actors.
  use-dispatcher = ""
}